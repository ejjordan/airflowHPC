
> mkdir ~/scalems
> cd ~/scalems
> export SCALEMS="$HOME/scalems"

### modules and conda env
> module load anaconda3
> module load gcc
> module load openmpi
> module load cuda
> conda create -n scalems python==3.10
> conda activate scalems


### spack and postgres

> git clone https://github.com/spack/spack
> export PATH="$PATH:$SCALEMS/spack/bin"
> . $SCALEMS/spack/share/spack/setup-env.sh
> spack install postgresql
> spack load postgresql

> initdb postgresql_db/data
> pg_ctl -D postgresql_db/data -l logfile start
> createdb -T template1 airflow_db
> psql airflow_db
```sql
CREATE USER airflow_user WITH PASSWORD 'airflow_pass';
GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;
GRANT ALL ON SCHEMA public TO airflow_user;
ALTER USER airflow_user SET search_path = public;
quit;
```


### airflow install
> pip install --upgrade pip wheel setuptools
> pip install radical.utils
> pip install radical.pilot
> pip install sphinx
> pip install apache-airflow[postgresql]
> conda install psycopg2

> cd $SCALEMS
> git clone https://github.com/ejjordan/airflowHPC.git
> cd airflowHPC/
> git checkout resource_rexee_radical
> pip install -r requirements.txt
> pip install -e .

# if you want to run performance analysis, use a specific RP branch and RA
> pip install radical.analytics
> git clone git@github.com:radical-cybertools/radical.pilot.git
> cd radical.pilot
> git checkout feature/command_msg
> pip install .
> cd ..


### gromacs and gmxapi install
> cd $SCALEMS
> wget https://ftp.gromacs.org/gromacs/gromacs-2024.4.tar.gz
> tar xf gromacs-2024.4.tar.gz
> rm gromacs-2024.4.tar.gz
> cd gromacs-2024.4/
> mkdir build
> cd build
> cmake -DCMAKE_INSTALL_PREFIX=`pwd`/../install -DGMX_BUILD_OWN_FFTW=ON -DGMX_MPI=ON -DGMX_GPU=CUDA ..
> make -j 8
> make install
> . $SCALEMS/gromacs-2024.4/install/bin/GMXRC.bash
> pip install --no-cache-dir gmxapi


# run test
cat > $SCALEMS/pilot_cfg.json <<EOF
{
    "resource" : "access.bridges2",
    "project"  : "dmr170002p",
    "queue"    : "GPU",
    "nodes"    : 2,
    "runtime"  : 20
}
EOF



# get an interactive job

#SBATCH --account=dmr170002p
#SBATCH --partition=GPU
#SBATCH --time=00:20:00
#SBATCH --nodes=2
#SBATCH --gpus=v100-32:16

> export SLURM_EXACT=1
> export SLURM_MEM_PER_NODE=0

> SCALEMS="$HOME/scalems"

> export OMP_PLACES=cores

> conda activate scalems

> # no idea why this is sometimes needed
> export PYTHONPATH=$SCALEMS/airflowHPC

> export RCT_PILOT_CFG=$SCALEMS/pilot_cfg.json
> export TMPDIR=$SCALEMS/tmp
> mkdir -p $TMPDIR

> export PATH="$PATH:$SCALEMS/spack/bin"
> . $SCALEMS/spack/share/spack/setup-env.sh

> export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow_user:airflow_pass@localhost/airflow_db"

> export AIRFLOW__CORE__EXECUTOR=airflowHPC.executors.resource_executor.ResourceExecutor
> export AIRFLOW__CORE__EXECUTOR=airflowHPC.executors.radical_executor.RadicalExecutor
> export AIRFLOW__CORE__LOAD_EXAMPLES=False
> export AIRFLOW__CORE__DAGS_FOLDER="$SCALEMS/airflowHPC/airflowHPC/dags/rct/"
> export AIRFLOW__HPC__CORES_PER_NODE=128
> export AIRFLOW__HPC__GPUS_PER_NODE=8
> export AIRFLOW__HPC__GPU_TYPE="nvidia"
> export AIRFLOW__HPC__MEM_PER_NODE=256
> export AIRFLOW__HPC__THREADS_PER_CORE=2
> export AIRFLOW_HOME=$SCALEMS/airflow_dir
> export RADICAL_UTILS_NO_ATFORK=1

> spack load postgresql
> pg_ctl -D $SCALEMS/postgresql_db/data/ -l $SCALEMS/postgresql_db/server.log start

> airflow scheduler -D
> airflow tasks clear rct_gmx_multi
> airflow dags trigger -v rct_gmx_multi
> airflow dags unpause rct_gmx_multi
> airflow dags details rct_gmx_multi

> # note down rp session ID (SID)
> radical-pilot-command <SID> dump
> cd <SID>
> ln -s <RP_PILOT_SANDBOX>/pilot.0000 .
> cd ..
> radical-analytics-inspect <SID>

