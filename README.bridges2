
> mkdir ~/scalems
> cd ~/scalems
> export SCALEMS="$HOME/scalems"

### modules and conda env
> module load anaconda3
> module load gcc openmpi
> conda create -n scalems python==3.10
> conda activate scalems


### spack and postgres

> git clone https://github.com/spack/spack
> export PATH="$PATH:$HOME/spack/bin"
> spack install postgresql
> spack load postgresql

> pg_ctl -D postgresql_db/data -l logfile start
> createdb -T template1 airflow_db
> psql airflow_db
...


### airflow install
> pip install --upgrade pip wheel setuptools
> pip install radical.pilot
> pip install radical.utils
> pip install sphinx
> pip install apache-airflow[postgresql]

> cd $SCALEMS
> git clone https://github.com/ejjordan/airflowHPC.git
> cd airflowHPC/
> git checkout resource_rexee_radical
> pip install -r requirements.txt 
> pip install -e .


### gromacs and gmxapi install
> cd $SCALEMS
> wget https://ftp.gromacs.org/gromacs/gromacs-2024.4.tar.gz
> tar xf gromacs-2024.4.tar.gz 
> rm gromacs-2024.4.tar.gz
> cd gromacs-2024.4/
> mkdir build
> cd build
> cmake -DCMAKE_INSTALL_PREFIX=`pwd`/../install -DGMX_BUILD_OWN_FFTW=ON  ..
> make -j 8
> make install
> . $SCALEMS/gromacs-2024.4/install/bin/GMXRC.bash


# run test
cat > $SCALEMS/pilot_cfg.json <<EOF
{
    "resource" : "access.bridges2",
    "project"  : "dmr170002p",
    "queue"    : "GPU",
    "nodes"    : 2,
  
    "runtime"  : 20
}
EOF



> cat > test.slurm <<EOF
#!/bin/bash -l

#SBATCH --account=dmr170002p
#SBATCH --partition=GPU
#SBATCH --time=00:10:00
#SBATCH --nodes=2
#SBATCH --gpus=v100-32:16


SCALEMS="$HOME/scalems"

export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK
export OMP_PLACES=cores

conda activate scalems

# no idea why this is sometimes needed
export PYTHONPATH=$HOME/airflowHPC

export RCT_PILOT_CFG=$SCALEMS/pilot_cfg.json
export TMPDIR=$SCALEMS/tmp

export PATH="$PATH:$HOME/spack/bin"
. $HOME/spack/share/spack/setup-env.sh

export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow_user:airflow_pass@localhost/airflow_db"
export AIRFLOW__CORE__EXECUTOR=airflowHPC.executors.resource_executor.ResourceExecutor
export AIRFLOW__CORE__LOAD_EXAMPLES=False
export AIRFLOW__CORE__DAGS_FOLDER="$SCALEMS/airflowHPC/airflowHPC/dags/"
export AIRFLOW__HPC__CORES_PER_NODE=128
export AIRFLOW__HPC__GPUS_PER_NODE=8
export AIRFLOW__HPC__GPU_TYPE="nvidia"
export AIRFLOW__HPC__MEM_PER_NODE=256
export AIRFLOW__HPC__THREADS_PER_CORE=2
export RADICAL_UTILS_NO_ATFORK=1

spack load postgresql
pg_ctl -D $SCALEMS/postgresql_db/data/ -l $SCALEMS/postgresql_db/server.log start

airflow dags backfill -s 2024-12-19 -v --reset-dagruns -y gmx_multi
EOF


> sbatch test.slurm

